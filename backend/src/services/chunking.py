import os
import re
from typing import List, Dict
from markdown_it import MarkdownIt
from bs4 import BeautifulSoup
from backend.src.models.rag_models import ContentChunk, Metadata

def chunk_markdown(file_path: str, chapter_id: str, chapter_title: str, chapter_metadata: dict) -> List[ContentChunk]:
    """
    Chunks a Markdown file into smaller, semantically meaningful pieces.
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        markdown_content = f.read()

    # Parse Markdown to HTML
    md = MarkdownIt()
    html_content = md.render(markdown_content)

    # Use BeautifulSoup to parse HTML and extract chunks
    soup = BeautifulSoup(html_content, 'html.parser')
    chunks = []
    current_chunk_content = []
    current_chunk_headings = []
    chunk_counter = 0

    # Basic chunking strategy: split by top-level headings or paragraphs
    for element in soup.children:
        if element.name and re.match(r'h[1-3]', element.name):
            if current_chunk_content:
                chunks.append(create_chunk(
                    current_chunk_content, current_chunk_headings,
                    file_path, chapter_id, chapter_title, chapter_metadata, chunk_counter
                ))
                chunk_counter += 1
                current_chunk_content = []
                current_chunk_headings = []
            current_chunk_headings.append(element.get_text(strip=True))
            current_chunk_content.append(str(element))
        elif element.name:
            current_chunk_content.append(str(element))
        
        # Add a simple condition to force chunk creation for very long content without headings
        if len(" ".join(current_chunk_content)) > 1000 and not re.match(r'h[1-3]', element.name):
            if current_chunk_content:
                chunks.append(create_chunk(
                    current_chunk_content, current_chunk_headings,
                    file_path, chapter_id, chapter_title, chapter_metadata, chunk_counter
                ))
                chunk_counter += 1
                current_chunk_content = []
                current_chunk_headings = []

    if current_chunk_content:
        chunks.append(create_chunk(
            current_chunk_content, current_chunk_headings,
            file_path, chapter_id, chapter_title, chapter_metadata, chunk_counter
        ))

    return chunks

def create_chunk(content_parts, headings, file_path, chapter_id, chapter_title, chapter_metadata, chunk_counter):
    full_content = " ".join(content_parts).strip()
    
    # Extract metadata fields
    module = chapter_metadata.get('module')
    week = chapter_metadata.get('week')
    keywords = chapter_metadata.get('keywords', [])

    metadata = Metadata(
        source_path=file_path,
        title=chapter_title,
        module=module,
        week=week,
        keywords=keywords,
        headings=headings
    )
    
    # Placeholder for vector - will be generated by embedding model
    vector = [0.0] * 768 # Example for a 768-dim embedding

    # Generate a simple chunk_id
    chunk_id = f"{chapter_id}-{chunk_counter}"

    return ContentChunk(
        chunk_id=chunk_id,
        chapter_id=chapter_id,
        content=full_content,
        vector=vector,
        metadata=metadata
    )

if __name__ == "__main__":
    # Example usage (for testing purposes)
    # This assumes you have a Docusaurus project in frontend/
    # And a basic docs structure like frontend/docs/introduction.md
    
    example_md_file = "frontend/docs/introduction.md"
    if os.path.exists(example_md_file):
        # Example: Mock Docusaurus frontmatter (normally parsed by Docusaurus)
        # For this example, we'll manually extract some to mimic it.
        mock_frontmatter = {
            'id': 'introduction',
            'title': 'Introduction to Physical AI',
            'description': 'Overview of the course and embodied intelligence.',
            'keywords': ['physical-ai', 'robotics']
        }
        
        # You'd typically get this from Docusaurus's own metadata parsing
        chapter_id = mock_frontmatter.get('id')
        chapter_title = mock_frontmatter.get('title')
        chapter_metadata = mock_frontmatter # Pass the entire dict for now

        if chapter_id and chapter_title:
            print(f"Chunking {example_md_file}...")
            chunks = chunk_markdown(example_md_file, chapter_id, chapter_title, chapter_metadata)
            for i, chunk in enumerate(chunks):
                print(f"--- Chunk {i} ---")
                print(f"ID: {chunk.chunk_id}")
                print(f"Chapter ID: {chunk.chapter_id}")
                print(f"Content (first 100 chars): {chunk.content[:100]}...")
                print(f"Metadata: {chunk.metadata.json()}")
                print("-" * 20)
        else:
            print(f"Could not extract ID or title from mock frontmatter for {example_md_file}")
    else:
        print(f"Example markdown file not found: {example_md_file}. Please create it to test chunking.")

